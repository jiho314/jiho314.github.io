---
title: "The Gap between ELBO and actual implementation"
toc: true
toc_sticky: true
toc_lable: "Main Contents"
use_math: true
categories:
- Generative
---

## The Gap between ELBO and actual Implementation

Evidence Lower BOund

$$
\begin{align}
 \log p_\theta(x) &= \int \log p_\theta(x,z)dz \\
&= \int \log \frac {p_\theta(x,z) q_{\phi}(z|x)}{q_{\phi}(z|x)}dz \\
& \geq \log E_{q_{\phi}(z|x)}[\log \frac{p_\theta(x,z)}{q_{\phi}(z|x)}] = \text{ELBO}
\end{align}
$$



Dividing ELBO into 'reconstruction term' and 'prior matching term'. 
$$
\begin{align}
\text{ELBO} &= E_{q_{\phi}(z|x)}[\log \frac{p(x,z)}{q_{\phi}(z|x)} ]\\
		&= E_{q_{\phi}(z|x)}[\log p_\theta(x|z)] -E_{q_{\phi}(z|x)}[\log \frac{p(z)}{q_{\phi}(z|x)}]\\
		&= \underbrace{E_{q_{\phi}(z|x)}[\log p_\theta(\hat{x}|z)]}_{\text{reconstruction term}} - \underbrace{D_{KL}(q_\phi(\hat{z}|x)||p(z))}_{\text{prior matching term}}
\end{align}
$$


Reconstruction term의 수식 그대로의 의미는,  
**'실제 $x$의 distribution $p$' 에 대한 '만들어진 $\hat{x}$ data들'의 likelihood** 이다.   
Prior matching term은 $z$에 대한 실제 prior와 $x$로부터 만들어진 $\hat{z}$ 의 distribution 차이를 의미한다. 

여기서 VAE 는 $p(z)=N(z;0,I)$ , z가 Gaussian을 따른다는 가정하에 '$-\text{ELBO}$'식을 Loss term으로 집어 넣는다고 한다. 하지만 VAE에서 사용되는 model은 두 개의 neural net( $\theta, \phi$) 뿐이다. 각각을 통해 $x, \space z$를 각각 생성할 수는 있지만, 그것들의 distribution을 어떻게 계산한다는 것인가?

 실질적으로 VAE에 적용되는 loss function은 아래와 같다. distribution에 대한 likelihood 를 의미하는 term은 단순히 실제 $x$ 와 reconstruct 된 $\hat{x}$ 간의 mse loss나 binary cross entropy loss로 치환되었다.

```python
def loss_function(recon_x, x, mu, logvar):
    reconstruction = F.mse_loss(recon_x,x)
    #reconstruction = F.binary_cross_entropy(recon_x, x.view(-1, 784))
    prior_matching = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return reconstruction + prior_matching
```

<br/>

이러한 이론적인 ELBO 와 실질적 Loss term간의 괴리는 그 사이에 추가된 가정들에 의한 것이었다.    
VAE에서는 ELBO에서 생성된 데이터들이 Gaussian / Bernoulli 라는 가정하에 ELBO를 계산해낸 것이다. 각각의 가정하에 Likelihood를 Maximize 하는 것은 각 MSE / BCE 로 최적화하는 것과 동치이다. (Gaussian 분포의 데이터를 MLE 하는 것 = 각각의 데이터로 MSE 하는 것)

Prior matching term 역시, 두  Gaussian 분포의 MLE는, 그 평균과 분산에 의해 계산된다는 결론 하에 구현된 것이다.  VAE의 encoding 과정에서($\phi: x \rightarrow z$ ),  z vector를 직접 만들지 않고 평균과 분산만 출력하는 것도 이러한 맥락에서 납득할 수 있는 것이다.



계산이 어려운 $\log p(x)$을 우회하기 위해 또 다른 $q$를 도입하여 만들어진 ELBO 역시, 그대로 계산을 할 수 없는 것이다. 따라서 ELBO라는 수식을 어떠한 가정을 가지고 계산을 해낼 것인가에 따라 아예 다른 모델이 될 수 있다. Generative 분야를 압도하는 Diffusion 역시, Hierarchical(계층적)-VAE 구조에 다른 가정을 적용하여 ELBO를 계산해낸 것으로 볼 수 있다.



 



<!--

ELBO를 최대화 함으로서 $p_\theta(x)$ 를 최대화 한다는 것은,  $\theta$ 로 생성된 data의 distribution   

-->

