---
title: "Gap between ELBO and it's implementation"
toc: true
toc_sticky: true
toc_lable: "Main Contents"
use_math: true
categories:
- Generative
---

## The Gap between ELBO and actual Implementation

Evidence Lower BOund derivation 
(Below is just a derivation among many other methods)

$$
\begin{align}
 \log p_\theta(x) &= \int \log p_\theta(x,z)dz \\
&= \int \log \frac {p_\theta(x,z) q_{\phi}(z|x)}{q_{\phi}(z|x)}dz \\
& \geq \log E_{q_{\phi}(z|x)}[\log \frac{p_\theta(x,z)}{q_{\phi}(z|x)}] = \text{ELBO}
\end{align}
$$


Dividing ELBO into 'reconstruction term' and 'prior matching term'. 
$$
\begin{align}
\text{ELBO} &= E_{q_{\phi}(z|x)}[\log \frac{p(x,z)}{q_{\phi}(z|x)} ]\\
		&= E_{q_{\phi}(z|x)}[\log p_\theta(x|z)] -E_{q_{\phi}(z|x)}[\log \frac{p(z)}{q_{\phi}(z|x)}]\\
		&= \underbrace{E_{q_{\phi}(z|x)}[\log p_\theta(\hat{x}|z)]}_{\text{reconstruction term}} - \underbrace{D_{KL}(q_\phi(\hat{z}|x)||p(z))}_{\text{prior matching term}}
\end{align}
$$
Reconstruction term의 수식 그대로의 의미는,

**'실제 $x$의 distribution $p$' 에 대한 '만들어진 $\hat{x}$ data들'의 likelihood** 이다.

Prior matching term은 $z$에 대한 실제 prior와 $x$로부터 만들어진 $\hat{z}$ 의 distribution 차이를 의미한다. 

여기서 VAE 는 $p(z)=N(z;0,I)$ , z가 Gaussian을 따른다는 가정하에 '$-\text{ELBO}$'식을 Loss term으로 집어 넣는다고 한다. 하지만 실질적으로 적용되는 loss function은 아래와 같다.

```python
def loss_function(recon_x, x, mu, logvar):
    reconstruction = F.mse_loss(recon_x,x)
    #reconstruction = F.binary_cross_entropy(recon_x, x.view(-1, 784))
    prior_matching = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return reconstruction + prior_matching
```

distribution에 대한 likelihood 를 의미하는 term은 단순히 실제 x 와 생성된 x 간의 mse loss나 binary cross entropy loss로 치환되었다.

<br/><br/>



이러한 이론적인 ELBO 와 실질적 Loss term간의 괴리는 그 사이의 추가된 가정들에 의한 것이었다.    
VAE에서는 ELBO에서 생성된 데이터들이 Gaussian / Bernoulli 라는 가정하에 ELBO를 계산해낸 것이다. 각각의 가정하에 Likelihood를 Maximize 하는 것은 각 MSE / BCE 로 최적화하는 것과 동치이다. (Gaussian 분포의 데이터를 MLE 하는 것 = 각각의 데이터로 MSE 하는 것)



계산이 어려운 term을 우회하기 위해 만들어진 ELBO 역시, 그대로 계산을 할 수 없는 것이다. 따라서 ELBO라는 수식을 어떠한 가정을 가지고 계산을 해낼 것인가에 따라 아예 다른 모델이 될 수 있는 것이다. Generative 분야를 압도하는 Diffusion 역시, Hierarchical(계층적)-VAE 구조에 다른 가정을 적용하여 ELBO를 계산해낸 것으로 볼 수 있다.



 

어떠한 가정을 

<!--

ELBO를 최대화 함으로서 $p_\theta(x)$ 를 최대화 한다는 것은,  $\theta$ 로 생성된 data의 distribution   

-->

여기서 우리가 알고 있는 것은 $z$ 로부터 $\hat{x}$ 을 생성해내는 neural net $\theta$ 와 $x$ 







The actual meaning of the reconstruction term is the $\textit{likelihood of the distribution of the generated data}$ 



However since $\theta$ is a generating neural network, and $\phi$ is encoding network, the 



