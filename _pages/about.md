---
permalink : /about/
---





<!-- style = "float: right;"/ 

<img style="text-align:left;" src = "https://jiho314.github.io/assets/imgs/me.png" width = "30%" >

-->

# ðŸ‘‹ Jiho, Park

Hi! I'm a senior undergraduate student majoring in Electrical & Electronic Engineering at Yonsei University, South Korea. My main research interests lie in Deep Learning, particularly in 3D Vision and scalable Robot Learning. <br/>

Previously, I was intrigued by the perception of 4D(dynamic 3D) world. Now, I'm deeply interested by how we can use this understanding to make informed decisions and take meaningful actions. Consequently, I am now refining research area within Robot Learning, aiming to integrate perception with actionable insights.<br/>

Below are the presentation materials related to robot learning that I've recently used. <br/>[SE(3)-Diffusion](https://jiho314.github.io/assets/presentation/SE(3)-DiffusionFields.pdf), [NeRF for Robotics](https://jiho314.github.io/assets/presentation/NeRF for Robotics.pdf)<br/>6	

<!--

Recently, my interests have evolved to encompass the development of meaningful decisions and actions derived from this perception, leading me toward the field of Robot Learning. Consequently, I am now refining research area within Robot Learning, aiming to integrate perception with actionable insights<br/>

Now, I'm intrigued by how we can use this understanding to make informed decisions and take meaningful actions, guiding me into the realm of Robot Learning. As a result, I'm currently fine-tuning my research within Robot Learning, aiming to merge the concepts of perception with actionable insights





Before, I was really into exploring how we perceive the dynamic 3D world, that fascinating 4D space. Lately, though, I've gotten excited about going a step furtherâ€”not just perceiving the world, but making smart decisions and taking actions based on what we see. This curiosity has steered me towards Robot Learning. So, I'm currently zeroing in on this cool intersection, trying to blend perception with practical, impactful actions.

Previously, I was deeply interested in understanding the perception of the dynamic 3D, or 4D, world. 

Now, I'm intrigued by how we can use this understanding to make informed decisions and take meaningful actions, guiding me into the realm of Robot Learning. As a result, I'm currently fine-tuning my research within Robot Learning, aiming to merge the concepts of perception with actionable insights.



However, I have since broadened my interests to include not only perception but also the generation of meaningful decision/actions through Robot Learning. As a result, I am currently refining my research focus within the field of Robot Learning.



Initially, my fascination lay with the perception of the 4D (Dynamic 3D) world. Recently, however, my interests have evolved to encompass the development of meaningful decisions and actions derived from this perception, leading me toward the field of Robot Learning. Consequently, I am now honing my research area within Robot Learning, aiming to integrate perception with actionable insights

I was previously interested in the perception of 4D(Dynamic 3D) world and now my research interest covering decision making and action not only perception. Therefore, now I'm narrowing down my research area among Robot Learning.

-->

<br/>





# Education

- B.S. of Electrical and Electronic Engineering, Yonsei University   
  *1st semester of 4th-year, GPA 4.10/4.3 ([transcript](https://jiho314.github.io/assets/transcript/transcript_4-1(kor)(norank).pdf))*  
  *(Mar. 2019 ~ Present)*
  - 1.5 years of absence due to military service
  

# Experiences

- Undergraduate Research Intern in [MLCS Lab](https://mlcs.yonsei.ac.kr/index.html) (ML & Control Lab) (Advisor. [*Jongeun Choi*](https://scholar.google.com/citations?user=Z-UlU3MAAAAJ&hl=en) )<br/> *(Mar. 2024 ~ Present)*
  - Developing a ROS-based system for robotic manipulation, incorporating the Kinova-Jaco arm, three RGBD cameras, and an AI model <br/>
  - Currently focusing on Camera Calibration and Point Cloud Registration
- Undergraduate Research Intern in [MIRLab](https://mirlab.yonsei.ac.kr/) (Multimodal AI Lab) (Advisor. Youngjae Yu) <br/>
  *(Aug. 2023 ~ Jan. 2024)*
  - Studied extensively in the field of 3D Vision, covering 3D Representations, Static & Dynamic 3D Scene Reconstruction
  - Developed a large-scale 3D Talking Head Dataset to facilitate talking head generation

# Work Experiences

- [Rebuilder AI](https://rebuilderai.com/) | Parttime Research Assistant *with YAI(Yonsei Artificial Intelligence)* <br/>
  *(Jul. 2023 ~ Sep. 2023)*
  - Background image generation for commercial product<br/> (built dataset and fine-tuned diffusion model)
  - Saliency-aware product segmentation 
- [Uaround](https://www.uaround.ai/) | Parttime Research Assistant *with DataScienceLab, Yonsei* <br/>
   *(May. 2022 ~ Jun. 2022)*
   - Face similarity modeling for virtual human 



# Projects

- 4D Head Reconstruction: Leveraging Semantic Information for *4D Gaussian Splatting*<br/>*(Oct. 2023 ~ Nov. 2023*) [*github_link*](https://github.com/whwjdqls/4D-Gaussian-Head.git)
- Camera Pose Estimation for Tensor Radiance Fields <br/>
  *(Sep. 2023 ~ Nov. 2023)* *[report_link](https://jiho314.github.io/assets/nope-tensorf.pdf)*
- [OOD Detection Research Project] Exploring the Generative Model for OOD Detection, with Hierarchical Self-Conditioned AutoEncoder <br/> *(Jun. 2023)* [***report_link***](https://jiho314.github.io/assets/MNIST_OOD_HSCAE.pdf)
- [Diffusion Model Web Application Project] Sketch&Prompt-to-Image using ControlNet <br/>*(Apr. 2023 ~ May. 2023) [github_link](https://github.com/devch1013/YAICON-Ditto)*
- Virtual Hand Drawing Simulator (Unity, Python communication project)  <br/>
  *(Nov. 2022 ~ Dec. 2022) [github link](https://github.com/jiho314/Unity_HandTracking_DeepLearning.git)*

# Military Service

- Korean Army, Honorable Discharge <br/>*(Sep. 2020 ~ Mar. 2022)*

# Scholarship

- Yonsei Veritas(High-academic Performers) Scholarship<br/>
  Honors: 2022-1, 2022-2 <br/>High Honors: 2023-1, 2023-2

- Hanseong Son Jae Han Scholarship Foundation  <br/>
  Hanseong Nobel scholarship, approx. 10,000 USD in total (awarded to up to 200 people every year) <br/>(*2017-2018*)

## Skills

- **Programming Languages:**  Python, C#(Unity), Verilog, C++

- **Languages:** Korean(Native), English(Fluent, TOEFL 105)



<!--



# Projects

- 4D Head Reconstruction: Leveraging Semantic information for 4D Gaussian Splatting<br/>*(Nov.2023*) [github_link](https://github.com/whwjdqls/4D-Gaussian-Head.git)
- [OOD Detection Research Project] Exploring the generative model for OOD Detection, with Hierarchical Self-Conditioned AutoEncoder <br/> *(Jun.2023)* [***report_link***](https://jiho314.github.io/assets/MNIST_OOD_HSCAE.pdf)
- [Diffusion Model Web Application Project] Sketch&Prompt-to-Image using ControlNet <br/>*(Apr.2023 ~ May.2023) [github_link](https://github.com/devch1013/YAICON-Ditto)*
- Virtual hand drawing simulator (Unity, Python communication project)  
  *(Nov. 2022 ~ Dec. 2022) [github link](https://github.com/jiho314/Unity_HandTracking_DeepLearning.git)*
- Cloth recommendation based on segmentation and data embedding   
  *(Mar. 2022 ~ Apr. 2022) [github_link](https://github.com/yejin109/MaskRCNN-Recommendation)*    

## Industry-academia cooperation projects

- Image Editing Services R&D<br/>
  *in YAI(AI club in Yonsei), with [Rebuilder AI](https://rebuilderai.com/)*
  - Background generation for commercial product
  - Saliency-aware product segmentation

- Highschool students' mathematical problem solving data clustering and analysis   
  *in Datasciencelab(Data science society in Yonsei), with Mathflat, freewheelin Inc.*   
  *(Oct. 2022 ~ Nov. 2022)*
- Service usage prediction and analysis   
  *in CSE-URP Yonsei, with JJAANN Co.*  
  *(Jul. 2022 ~ Aug. 2022)*
- Virtual face similarity modeling  
  *in Datasciencelab(Data science society in Yonsei), with MetaSoul, Uaround Co., Ltd*  
  *(May. 2022 ~ June. 2022)*



# Internship

- Undergraduate Intern in [MIRLab](https://mirlab.yonsei.ac.kr/) <br/>
  *(Jul.2023 ~ present)*
- CSE(Computational Science and Engineering)-URP Yonsei University in [MIDaS Lab](https://sites.google.com/site/midasyonsei)  
  *(Jul.2022 ~ Aug. 2022)*



# Military Service

- Korean Army, Honorable Discharge  
  *(Sep. 2020 ~ Mar. 2022)*

# Scholarship

- Yonsei Veritas(High-academic Performers) Scholarship   
  *(2022-2, 2023-1, 2023-2)*

# Studied Paper

### - Computer Vision

>  [VGGNet] Very Deep Convolutional Networks for Large-Scale Image Recognition [*vggnet_review*](https://jiho314.github.io/assets/paper-review/vggnet_review.pdf) <br/>[ResNet] Deep Residual Learning for Image Recognition [*resnet_review*](https://jiho314.github.io/assets/paper-review/resnet_review.pdf) <br/>[SpatialTransformer] Spatial Transformer Networks [*spatialtransformer_review*](https://jiho314.github.io/assets/paper-review/spatialtransformer_review.pdf) <br/>[FSRCNN] Accelerating the Super-Resolution Convolutional Neural Network [*fsrcnn_review*](https://jiho314.github.io/assets/paper-review/fsrcnn_review.pdf) <br/>[FCN] Fully Convolutional Networks for Semantic Segmentation  [*fcn_review*](https://jiho314.github.io/assets/paper-review/fcn_review.pdf) <br/>[DilatedConv] Multi-Scale Context Aggregation by Dilated Convolutions [*dilatedconv_review*](https://jiho314.github.io/assets/paper-review/dilatedconv_review.pdf) <br/>[YOLO] You Only Look Once: Unified, Real-Time Object Detection  [*yolo_review*](https://jiho314.github.io/assets/paper-review/yolo_review.pdf) <br/>
>  [Style Transfer] Image Style Transfer Using Convolutional Neural Networks [styletransfer_review](https://jiho314.github.io/assets/paper-review/styletransfer_review.pdf) <br/>Perceptual Losses for Real-Time Style Transfer and Super-Resolution [perceptualloss_review](https://jiho314.github.io/assets/paper-review/styletransfer_review.pdf) <br/>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization [gradcam_review](https://jiho314.github.io/assets/paper-review/gradcam_review.pdf) <br/>

### - Generative

> [GAN] Generative Adversarial Nets [gan_review](https://jiho314.github.io/assets/paper-review/gan_review.pdf) <br/>[cGAN] Conditonal Generative Adversarial Nets [cgan_review](https://jiho314.github.io/assets/paper-review/gradcam_review.pdf) <br/>[pix2pix] Image-to-Image Translation with Conditional Adversarial Networks <br/>
> [CycleGAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [*cyclegan_ppt* ](https://jiho314.github.io/assets/presentation/cyclegan_ppt.pdf)<br/>[DefenseGAN] Protecting Classifiers against Adversarial Attacks using Generative Models<br/>[DallE1] Zero-Shot Text-to-Image Generation<br/>

### - Diffusion

> Understanding Diffusion Models: A Unified Perspective [***Diffusion_Presentation***](https://jiho314.github.io/assets/presentation/diffusion_ppt.pdf)<br/>[DDPM] Denoising Diffusion Probabilistic Models <br/>[Latent Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models <br/>[ControlNet] Adding Conditional Control to Text-to-Image Diffusion Models <br/>

### - 3D

> NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis [*nerf_ppt*](https://jiho314.github.io/assets/presentation/nerf_ppt.pdf) <br/>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction <br/>Neural 3D Scene Reconstruction with the Manhattan-world Assumption [ManhattanSDF_ppt](https://jiho314.github.io/assets/presentation/ManhattanSDF_ppt.pdf)<br/>TensoRF: Tensorial Radiance Fields <br/>

### - NLP

> Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling [gru_review](https://jiho314.github.io/assets/paper-review/gru_review.pdf)<br/>Sequence to Sequence Learning with Neural Networks <br/>
> Attention is all you need



-->

