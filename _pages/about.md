---
permalink : /about/

---





<!-- style = "float: right;"/ 

<img style="text-align:left;" src = "https://jiho314.github.io/assets/imgs/me.png" width = "30%" >

-->



# ðŸ‘‹ Jiho, Park

<img style="float: right;" src = "https://jiho314.github.io/assets/imgs/me.png" width = "30%" >

Hi! I'm a senior undergraduate student majoring in Electrical & Electronic Engineering at Yonsei University, South Korea. Currently, I am conducting research on the topic of Coarse-to-fine Behavior Cloning with Action Sequence Quantization. <br/>

Previously, my intrigue was centered on the research field exploring the understanding of the 4D (dynamic 3D) world. Now, I'm deeply interested in how we can use this perception to make informed decisions and take meaningful actions. Consequently, I am refining my research area within Robot Learning, aiming to integrate perception with actionable insights.<br/>

<!--

I have wide research interest in robot learning and I'm currently doing research on Behavior Cloning (Coarse-to-fine(hierarchical) behavior transformer with Action sequence quantization). (Planning to finish this work by August)

\- Self-Supervised Learning on Multi-Sensory Data of Agent w/ proprioception for RL (e.g. https://pbecker93.github.io/coral_pp/)
\- Learning more(getting actionable insights) from observation (but not like using 3D representation, more human-like understanding), e.g. https://edwardshu.com/scaffolder/)
\- Learning(BC/RL) for Complex Agent(e.g. humanoid, Multi-sensory Learning)
\- Hierarchical RL / Planning, Offline-RL, ...

-->



Additionally, I'm excited to join UT Austin as an exchange student in Fall 2024!<br/>
(funded by the Korea-U.S. Advanced Technology Youth Exchange Scholarship)<br/>

<br/>

Please refer to my recent [cv](https://jiho314.github.io/assets/cv.pdf) for more details!

<!--

Recently, my interests have evolved to encompass the development of meaningful decisions and actions derived from this perception, leading me toward the field of Robot Learning. Consequently, I am now refining research area within Robot Learning, aiming to integrate perception with actionable insights<br/>

Now, I'm intrigued by how we can use this understanding to make informed decisions and take meaningful actions, guiding me into the realm of Robot Learning. As a result, I'm currently fine-tuning my research within Robot Learning, aiming to merge the concepts of perception with actionable insights





Before, I was really into exploring how we perceive the dynamic 3D world, that fascinating 4D space. Lately, though, I've gotten excited about going a step furtherâ€”not just perceiving the world, but making smart decisions and taking actions based on what we see. This curiosity has steered me towards Robot Learning. So, I'm currently zeroing in on this cool intersection, trying to blend perception with practical, impactful actions.

Previously, I was deeply interested in understanding the perception of the dynamic 3D, or 4D, world. 

Now, I'm intrigued by how we can use this understanding to make informed decisions and take meaningful actions, guiding me into the realm of Robot Learning. As a result, I'm currently fine-tuning my research within Robot Learning, aiming to merge the concepts of perception with actionable insights.



However, I have since broadened my interests to include not only perception but also the generation of meaningful decision/actions through Robot Learning. As a result, I am currently refining my research focus within the field of Robot Learning.



Initially, my fascination lay with the perception of the 4D (Dynamic 3D) world. Recently, however, my interests have evolved to encompass the development of meaningful decisions and actions derived from this perception, leading me toward the field of Robot Learning. Consequently, I am now honing my research area within Robot Learning, aiming to integrate perception with actionable insights

I was previously interested in the perception of 4D(Dynamic 3D) world and now my research interest covering decision making and action not only perception. Therefore, now I'm narrowing down my research area among Robot Learning.



Selected Courses: Computer Vision(Diffusion Models, Camera System)(A+), Deep Learning Lab(A+), LinearAlgbera for EEE(A+), Mathematics for EEE(Convex Optimization)(A+)  

-->



<!--

# Education

- B.S. of Electrical and Electronic Engineering, Yonsei University   
  *1st semester of 4th-year, GPA 4.10/4.3 ([transcript](https://jiho314.github.io/assets/transcript/transcript_JIHO PARK.pdf))*  
  *(Mar. 2019 ~  Present)*
  - 1.5 years of absence due to military service
  

# Research Experiences

- Undergraduate Research Intern in [MLCS Lab](https://mlcs.yonsei.ac.kr/index.html) (ML & Control Lab) (Advisor. [*Jongeun Choi*](https://scholar.google.com/citations?user=Z-UlU3MAAAAJ&hl=en) )<br/> *(Apr. 2024 ~ Present)*
  - Developed a ROS-based system for robotic manipulation, incorporating the Kinova-Jaco arm, three RGBD cameras, and an AI model <br/>
- Undergraduate Research Intern in [MIRLab]() (Multimodal AI Lab) <br/>
  *(Aug. 2023 ~ Jan. 2024)*
  - Studied extensively in the field of 3D Vision, covering 3D Representations, Static & Dynamic 3D Scene Reconstruction
  - Developed a large-scale 3D Talking Head Dataset to facilitate talking head generation

# Work Experiences

- [Rebuilder AI](https://rebuilderai.com/) | Parttime Research Assistant *with YAI(Yonsei Artificial Intelligence)* <br/>
  *(Jul. 2023 ~ Sep. 2023)*
  - Background image generation for commercial product<br/> (built dataset and fine-tuned diffusion model)
  - Saliency-aware product segmentation 
- [Uaround](https://www.uaround.ai/) | Parttime Research Assistant *with DataScienceLab, Yonsei* <br/>
   *(May. 2022 ~ Jun. 2022)*
   - Face similarity modeling for virtual human 



# Projects

- 4D Head Reconstruction: Leveraging Semantic Information for *4D Gaussian Splatting*<br/>*(Oct. 2023 ~ Nov. 2023*) [*github_link*](https://github.com/whwjdqls/4D-Gaussian-Head.git)
- Camera Pose Estimation for Tensor Radiance Fields <br/>
  *(Sep. 2023 ~ Nov. 2023)* *[report_link](https://jiho314.github.io/assets/nope-tensorf.pdf)*
- [OOD Detection Research Project] Exploring the Generative Model for OOD Detection, with Hierarchical Self-Conditioned AutoEncoder <br/> *(Jun. 2023)* [***report_link***](https://jiho314.github.io/assets/MNIST_OOD_HSCAE.pdf)
- [Diffusion Model Web Application Project] Sketch&Prompt-to-Image using ControlNet <br/>*(Apr. 2023 ~ May. 2023) [github_link](https://github.com/devch1013/YAICON-Ditto)*
- Virtual Hand Drawing Simulator (Unity, Python communication project)  <br/>
  *(Nov. 2022 ~ Dec. 2022) [github link](https://github.com/jiho314/Unity_HandTracking_DeepLearning.git)*

# Scholarship

- Korea-U.S. Advanced Technology Youth Exchange Scholarship<br/>by Korea Institute for Advancement of Technology(KIAT) <br/>
  approx. 9000 USD for single semester<br/>*(Fall 2024)*
- Yonsei Veritas(High-academic Performers) Scholarship<br/>
  Honors: Spring 2022, Fall 2022<br/>High Honors: Spring 2023, Fall 2023 
- Hanseong Son Jae Han Nobel Scholarship <br/>
  approx. 10,000 USD in total (awarded to up to 200 people every year) <br/>(*2017-2018*)









-->





















<!--



# Projects

- 4D Head Reconstruction: Leveraging Semantic information for 4D Gaussian Splatting<br/>*(Nov.2023*) [github_link](https://github.com/whwjdqls/4D-Gaussian-Head.git)
- [OOD Detection Research Project] Exploring the generative model for OOD Detection, with Hierarchical Self-Conditioned AutoEncoder <br/> *(Jun.2023)* [***report_link***](https://jiho314.github.io/assets/MNIST_OOD_HSCAE.pdf)
- [Diffusion Model Web Application Project] Sketch&Prompt-to-Image using ControlNet <br/>*(Apr.2023 ~ May.2023) [github_link](https://github.com/devch1013/YAICON-Ditto)*
- Virtual hand drawing simulator (Unity, Python communication project)  
  *(Nov. 2022 ~ Dec. 2022) [github link](https://github.com/jiho314/Unity_HandTracking_DeepLearning.git)*
- Cloth recommendation based on segmentation and data embedding   
  *(Mar. 2022 ~ Apr. 2022) [github_link](https://github.com/yejin109/MaskRCNN-Recommendation)*    

## Industry-academia cooperation projects

- Image Editing Services R&D<br/>
  *in YAI(AI club in Yonsei), with [Rebuilder AI](https://rebuilderai.com/)*
  - Background generation for commercial product
  - Saliency-aware product segmentation

- Highschool students' mathematical problem solving data clustering and analysis   
  *in Datasciencelab(Data science society in Yonsei), with Mathflat, freewheelin Inc.*   
  *(Oct. 2022 ~ Nov. 2022)*
- Service usage prediction and analysis   
  *in CSE-URP Yonsei, with JJAANN Co.*  
  *(Jul. 2022 ~ Aug. 2022)*
- Virtual face similarity modeling  
  *in Datasciencelab(Data science society in Yonsei), with MetaSoul, Uaround Co., Ltd*  
  *(May. 2022 ~ June. 2022)*



# Internship

- Undergraduate Intern in [MIRLab](https://mirlab.yonsei.ac.kr/) <br/>
  *(Jul.2023 ~ present)*
- CSE(Computational Science and Engineering)-URP Yonsei University in [MIDaS Lab](https://sites.google.com/site/midasyonsei)  
  *(Jul.2022 ~ Aug. 2022)*



# Military Service

- Korean Army, Honorable Discharge  
  *(Sep. 2020 ~ Mar. 2022)*

# Scholarship

- Yonsei Veritas(High-academic Performers) Scholarship   
  *(2022-2, 2023-1, 2023-2)*

# Studied Paper

### - Computer Vision

>  [VGGNet] Very Deep Convolutional Networks for Large-Scale Image Recognition [*vggnet_review*](https://jiho314.github.io/assets/paper-review/vggnet_review.pdf) <br/>[ResNet] Deep Residual Learning for Image Recognition [*resnet_review*](https://jiho314.github.io/assets/paper-review/resnet_review.pdf) <br/>[SpatialTransformer] Spatial Transformer Networks [*spatialtransformer_review*](https://jiho314.github.io/assets/paper-review/spatialtransformer_review.pdf) <br/>[FSRCNN] Accelerating the Super-Resolution Convolutional Neural Network [*fsrcnn_review*](https://jiho314.github.io/assets/paper-review/fsrcnn_review.pdf) <br/>[FCN] Fully Convolutional Networks for Semantic Segmentation  [*fcn_review*](https://jiho314.github.io/assets/paper-review/fcn_review.pdf) <br/>[DilatedConv] Multi-Scale Context Aggregation by Dilated Convolutions [*dilatedconv_review*](https://jiho314.github.io/assets/paper-review/dilatedconv_review.pdf) <br/>[YOLO] You Only Look Once: Unified, Real-Time Object Detection  [*yolo_review*](https://jiho314.github.io/assets/paper-review/yolo_review.pdf) <br/>
>  [Style Transfer] Image Style Transfer Using Convolutional Neural Networks [styletransfer_review](https://jiho314.github.io/assets/paper-review/styletransfer_review.pdf) <br/>Perceptual Losses for Real-Time Style Transfer and Super-Resolution [perceptualloss_review](https://jiho314.github.io/assets/paper-review/styletransfer_review.pdf) <br/>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization [gradcam_review](https://jiho314.github.io/assets/paper-review/gradcam_review.pdf) <br/>

### - Generative

> [GAN] Generative Adversarial Nets [gan_review](https://jiho314.github.io/assets/paper-review/gan_review.pdf) <br/>[cGAN] Conditonal Generative Adversarial Nets [cgan_review](https://jiho314.github.io/assets/paper-review/gradcam_review.pdf) <br/>[pix2pix] Image-to-Image Translation with Conditional Adversarial Networks <br/>
> [CycleGAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [*cyclegan_ppt* ](https://jiho314.github.io/assets/presentation/cyclegan_ppt.pdf)<br/>[DefenseGAN] Protecting Classifiers against Adversarial Attacks using Generative Models<br/>[DallE1] Zero-Shot Text-to-Image Generation<br/>

### - Diffusion

> Understanding Diffusion Models: A Unified Perspective [***Diffusion_Presentation***](https://jiho314.github.io/assets/presentation/diffusion_ppt.pdf)<br/>[DDPM] Denoising Diffusion Probabilistic Models <br/>[Latent Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models <br/>[ControlNet] Adding Conditional Control to Text-to-Image Diffusion Models <br/>

### - 3D

> NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis [*nerf_ppt*](https://jiho314.github.io/assets/presentation/nerf_ppt.pdf) <br/>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction <br/>Neural 3D Scene Reconstruction with the Manhattan-world Assumption [ManhattanSDF_ppt](https://jiho314.github.io/assets/presentation/ManhattanSDF_ppt.pdf)<br/>TensoRF: Tensorial Radiance Fields <br/>

### - NLP

> Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling [gru_review](https://jiho314.github.io/assets/paper-review/gru_review.pdf)<br/>Sequence to Sequence Learning with Neural Networks <br/>
> Attention is all you need



-->

